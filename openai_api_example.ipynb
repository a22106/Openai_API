{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import subprocess\n",
    "import requests\n",
    "from typing import List, Optional\n",
    "from pprint import pprint\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GPT_MODEL = [\"text-davinci-003\"]\n",
    "\n",
    "def print_response(response):\n",
    "    if response.status_code == 200:\n",
    "        print(response.json())\n",
    "        return response.json()\n",
    "    else:\n",
    "        error = response.json()\n",
    "        print(f\"Error: {error['error']}\")\n",
    "        return error\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create completion\n",
    "- DOC url: https://platform.openai.com/docs/api-reference/completions/create"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/completions\", \\\n",
    "    headers=headers, json={\n",
    "        \"model\": \"text-davinci-003\",\n",
    "        \"prompt\": \"Say this is a test\",\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 1000\n",
    "    }\n",
    ")\n",
    "\n",
    "def create_completion(prompt:str, suffix:str = None, model: str = \"text-davinci-003\", max_tokens: Optional[int] = 500, temperature: float = 1, top_p: Optional[int] = 1, n: Optional[int] = 1,  stream: Optional[bool] = False, echo: Optional[bool] = False):\n",
    "    \"\"\"\n",
    "    https://platform.openai.com/docs/api-reference/completions/create\n",
    "    ##### Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position.\n",
    "\n",
    "    Args:\n",
    "        - prompt (str): \n",
    "            - The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays. \n",
    "            Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document. \n",
    "        \n",
    "        - suffix (str): \n",
    "            - The suffix that comes after a completion of inserted text.\n",
    "        \n",
    "        - model (str, required): \n",
    "            - ID of the model to use. You can use the **List models** API to see all of your available models, or see our **Model overview** for descriptions of them. Defaults to \"text-davinci-003\".\n",
    "        \n",
    "        - max_tokens (int, optional, Default: 500): \n",
    "            - The maximum number of tokens to generate in the completion.\n",
    "                The token count of your prompt plus `max_tokens` cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096). Defaults to 500.\n",
    "        \n",
    "        - temperature (float, optional): \n",
    "            - What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
    "            We generally recommend altering this or `top_p` but not both. Defaults to 0.\n",
    "            \n",
    "        - top_p (float, optional):\n",
    "            - An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
    "            - We generally recommend altering this or `temperature` but not both. Defaults to 1.\n",
    "            \n",
    "        - n (int, optional, Default: 1):\n",
    "            - The number of completions to generate. Defaults to 1. \n",
    "        \n",
    "        - stream (bool, optional): \n",
    "            - Whether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message. Defaults to False.\n",
    "        \n",
    "        - echo (bool, optional): _description_. Defaults to False.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    response = requests.post(\"https://api.openai.com/v1/completions\", \\\n",
    "        headers=headers, json={\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"suffix\": suffix,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"stream\": stream,\n",
    "            \"echo\": echo,\n",
    "            \"stop\": suffix,\n",
    "            \"n\": n,\n",
    "            \"top_p\": top_p\n",
    "        }\n",
    "    )\n",
    "    return response\n",
    "    \n",
    "def edit_prompt(instruction: str, input: Optional[str] = None, model: str = \"text-davinci-edit-001\", num: Optional[int] = 1, temperature: Optional[float] = 1, top_p: Optional[float] = 1):\n",
    "    \"\"\"#### Given a prompt and an instruction, the model will return an edited version of the prompt.\n",
    "    https://platform.openai.com/docs/api-reference/edits\n",
    "    Args:\n",
    "        - instruction (str, required): \n",
    "            - The instruction that tells the model how to edit the prompt.\n",
    "            \n",
    "        - input (Optional[str], optional): \n",
    "            - The input text to use as a starting point for the edit. Defaults to None.\n",
    "        \n",
    "        - model (str, required): \n",
    "            - ID of the model to use. You can use the `text-davinci-edit-001` or `code-davinci-edit-001` model with this endpoint. Defaults to \"text-davinci-edit-001\".\n",
    "\n",
    "        - n (Optional[int], optional): \n",
    "            - How many edits to generate for the input and instruction. Defaults to 1.\n",
    "\n",
    "        - temperature (Optional[float], optional): \n",
    "            - What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
    "            - We generally recommend altering this or `top_p` but not both. Defaults to 1.\n",
    "        \n",
    "        - top_p (Optional[float], optional):\n",
    "            - An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
    "            - We generally recommend altering this or `temperature` but not both. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        - response\n",
    "    \"\"\"    \n",
    "    response = requests.post(\"https://api.openai.com/v1/edits\", \\\n",
    "        headers=headers, json={\n",
    "            \"model\": model,\n",
    "            \"input\": input,\n",
    "            \"instruction\": instruction,\n",
    "            \"n\": num,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p\n",
    "        }\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load kr_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['landing', 'commonErrors', 'buttons', 'navbar', 'generate-page', 'upscaler', 'purchase', 'lg-profile', 'lg-payments', 'settings', 'feedback', 'sm-profile', 'sm-payments', 'sm-settings', 'sm-security', 'footer', 'login', 'signup', 'successfull', 'verification'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./test/common_kr.json\", \"r\") as f:\n",
    "    kr_common = json.load(f)\n",
    "pprint(kr_common.keys())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_text = \"\"\"\n",
    "The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\n",
    "\n",
    "Human: Hello, who are you?\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "res_comp = create_completion(completion_text, model=\"text-davinci-003\", max_tokens=500, temperature=0.2, top_p=1, n=1, stream=False, echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'choices': [ { 'finish_reason': 'stop',\n",
      "                 'index': 0,\n",
      "                 'logprobs': None,\n",
      "                 'text': \"Hello there! I'm an AI assistant. I'm here to help \"\n",
      "                         'you with any questions or tasks you may have.'}],\n",
      "  'created': 1675942208,\n",
      "  'id': 'cmpl-6hzPs4EX8OP1LI1wURc6t4vVkyPKZ',\n",
      "  'model': 'text-davinci-003',\n",
      "  'object': 'text_completion',\n",
      "  'usage': {'completion_tokens': 24, 'prompt_tokens': 38, 'total_tokens': 62}}\n"
     ]
    }
   ],
   "source": [
    "pprint(res_comp.json(), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'âœ¨ðŸŒŒðŸ¤´ðŸ‘¸'\n"
     ]
    }
   ],
   "source": [
    "completion_text_conversation = \"\"\"\n",
    "Convert movie titles into emoji.\n",
    "\n",
    "Back to the Future: ðŸ‘¨ðŸ‘´ðŸš—ðŸ•’ \n",
    "Batman: ðŸ¤µðŸ¦‡ \n",
    "Transformers: ðŸš—ðŸ¤– \n",
    "Star Wars:\n",
    "\"\"\"\n",
    "res_comp = create_completion(completion_text_conversation, model=\"text-davinci-003\", max_tokens=500, temperature=0.2, top_p=1, n=1, stream=False, echo=False)\n",
    "pprint(res_comp.json()['choices'][0]['text'], indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " 'Jupiter is the fifth planet from the Sun and the biggest in our Solar '\n",
      " 'System. It is very bright and can be seen in the night sky. It has been '\n",
      " 'known since ancient times and is named after the Roman god Jupiter. It is so '\n",
      " 'bright that it can cast shadows on Earth.')\n"
     ]
    }
   ],
   "source": [
    "completion_text_summarization = \"\"\"\n",
    "Summarize this for a second-grade student:\n",
    "\n",
    "Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "res_comp = create_completion(completion_text_summarization, model=\"text-davinci-003\", max_tokens=500, temperature=0.2, top_p=1, n=1, stream=False, echo=False)\n",
    "pprint(res_comp.json()['choices'][0]['text'], indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'choices': [ { 'finish_reason': 'stop',\n",
      "                 'index': 0,\n",
      "                 'logprobs': None,\n",
      "                 'text': '\\n'\n",
      "                         '### Parade is more Fun than Rides\\n'\n",
      "                         '\\n'\n",
      "                         'Going to an amusement park is always an exciting '\n",
      "                         'experience. But for me, the best part is the parade. '\n",
      "                         'The colorful floats, the music, and the performers '\n",
      "                         'make it a truly magical experience. The rides are '\n",
      "                         \"fun, but the parade is much more entertaining. It's \"\n",
      "                         'a great way to get the whole family involved in the '\n",
      "                         \"fun. Plus, it's a great way to get a break from the \"\n",
      "                         \"heat and the crowds. So, if you're looking for a fun \"\n",
      "                         \"day out, don't forget to check out the parade!\"}],\n",
      "  'created': 1675942794,\n",
      "  'id': 'cmpl-6hzZK1YgBVGKvqoxzKdNVbQ4e7hsQ',\n",
      "  'model': 'text-davinci-003',\n",
      "  'object': 'text_completion',\n",
      "  'usage': {'completion_tokens': 117, 'prompt_tokens': 64, 'total_tokens': 181}}\n"
     ]
    }
   ],
   "source": [
    "completion_text_episode = \"\"\"\n",
    "Write an episode for 2-minute speaking practice English under 100 words. This episode is for blog posts in markdown format. heading has 3 hashes. You are given main subject and subtopic as [main subject, subtopic]\n",
    "\n",
    "[Amusement Park, Parade is more fun than rides]: \n",
    "\"\"\"\n",
    "res_comp = create_completion(completion_text_episode, model=\"text-davinci-003\", max_tokens=500, temperature=0.2, top_p=1, n=1, stream=False, echo=False)\n",
    "pprint(res_comp.json(), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'choices': [ { 'finish_reason': 'stop',\n",
      "                 'index': 0,\n",
      "                 'logprobs': None,\n",
      "                 'text': '\\n'\n",
      "                         '1. Open your terminal and navigate to the local '\n",
      "                         'directory where you want to store the repository.\\n'\n",
      "                         '\\n'\n",
      "                         '2. Run the command `git init` to initialize a new '\n",
      "                         'local git repository.\\n'\n",
      "                         '\\n'\n",
      "                         '3. Run the command `git remote add origin '\n",
      "                         '<url-of-your-github-repository>` to connect your '\n",
      "                         'local repository to the remote Github repository.\\n'\n",
      "                         '\\n'\n",
      "                         '4. Run the command `git pull origin master` to pull '\n",
      "                         'the contents of the remote repository into your '\n",
      "                         'local repository.\\n'\n",
      "                         '\\n'\n",
      "                         '5. Run the command `git push -u origin master` to '\n",
      "                         'push your local changes to the remote repository.'}],\n",
      "  'created': 1675943467,\n",
      "  'id': 'cmpl-6hzkBavXPgCyUlGE09dCpPMaLVYVh',\n",
      "  'model': 'text-davinci-003',\n",
      "  'object': 'text_completion',\n",
      "  'usage': {'completion_tokens': 127, 'prompt_tokens': 26, 'total_tokens': 153}}\n"
     ]
    }
   ],
   "source": [
    "completion_text = \"\"\"\n",
    "I just created a new Github repository. How can I connect this repository to my local git?\n",
    "\n",
    "Description: \n",
    "\"\"\"\n",
    "res_comp = create_completion(completion_text, model=\"text-davinci-003\", max_tokens=500, temperature=0.2, top_p=1, n=1, stream=False, echo=False)\n",
    "pprint(res_comp.json(), indent=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_code = \"\"\"\n",
    "How come I use my phone to check the weather? My teacher said that I should use.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object': 'edit', 'created': 1675941262, 'choices': [{'text': '\\nHow come I use my phone to check the weather? My teacher said that I should use the computer.\\n', 'index': 0}], 'usage': {'prompt_tokens': 41, 'completion_tokens': 52, 'total_tokens': 93}}\n",
      "\n",
      "How come I use my phone to check the weather? My teacher said that I should use the computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edit_res = edit_prompt(\"Fix the sentence if the it looks awkward.\", input=example_code, model=\"text-davinci-edit-001\", num=1, temperature=0.2, top_p=1)\n",
    "print(edit_res.json())\n",
    "print(''.join(edit_res.json()['choices'][0]['text']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imezy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52f7a0d066df59d9a6456dd3e1fe0c99c800b0fde99579ffc1629528aaf75ba1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
